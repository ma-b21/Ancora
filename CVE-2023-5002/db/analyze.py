from pprint import pprint
import re
import os
import pandas as pd
pd.set_option('future.no_silent_downcasting', True)  
import json

DEBUG = False

if DEBUG:
    pd.set_option('display.max_columns', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_colwidth', None)

data = pd.read_json("./capture/capture.jsonl", lines=True, dtype={
    "proc.vpid": str, "proc.cvpid": str, "proc.pvpid": str, "thread.vtid": str
    , "thread.pvtid": str, "thread.cvtid": str, "evt.num": str, "evt.arg.data": str})

def analyze_postgres_data(data: pd.DataFrame) -> pd.DataFrame:
    log_re = re.compile(
        r".*?\[([0-9\.]+\(\d+\))\].*?LOG:  statement: (.*)", re.DOTALL
    )

    data = data.copy()
    # 如果evt.arg.data为None，去掉该行
    data = data[data["evt.arg.data"].notna()]
    # 非数据库日志丢弃
    data = data[data["evt.arg.data"].apply(
        lambda x: "LOG:" in x
    )]
    # 提取数据库连接
    data["client_tuple"] = data["evt.arg.data"].apply(
        lambda x: log_re.match(x).group(1) if log_re.match(x) else None
    )
    def get_statement(x: str):
        x = x.replace("..", " ").strip(". ").replace("( ", "(")
        x = re.sub(r"\s+", " ", x)
        return x
    data["statement"] = data["evt.arg.data"].apply(
        lambda x: get_statement(log_re.match(x).group(2)) if log_re.match(x) else None
    )
    # data["details"] = data["evt.arg.data"].apply(
    #     lambda x: log_re.match(x).group(3) if log_re.match(x) else None
    # )

    return data


def extract_tuple_for_single_request(data: pd.DataFrame) -> pd.DataFrame:
    # 提取数据库连接
    connection_re = re.compile(
        r"(\d+.\d+.\d+.\d+):(\d+)->(\d+.\d+.\d+.\d+):5432"
    )
    # fd.name为None的行补为空字符串
    try:
        data["fd.name"] = data["fd.name"].fillna("NULL")
    except:
        # print(data)
        raise
    data["client_tuple"] = data["fd.name"].apply(
        lambda x: f"{connection_re.match(x).group(1)}({connection_re.match(x).group(2)})"
        if connection_re.match(x) else None
    )
    data["client_tuple"] = data["client_tuple"].ffill()
    data["client_tuple"] = data["client_tuple"].bfill()
    return data


if __name__ == "__main__":
    os.system("rm -f ./request_data/*")
    # data按thread.tid分组
    server_data = data[data["proc.name"] == "pgadmin4"]
    server_data = server_data[server_data["thread.pvtid"] == "1"]
    grouped_data = server_data.groupby("thread.vtid", sort=False)

    postgres_data = data[data["proc.name"] == "postgres"]
    postgres_data = analyze_postgres_data(postgres_data)
    # postgres_data.to_json("postgres_data.jsonl", orient="records", lines=True)
    # 将single_process_data每行数据作为json格式输出
    # single_process_data.to_json("single_process_data.jsonl", orient="records", lines=True)
    length = len(grouped_data)
    print(f"length: {length}")
    aggregated_data = pd.DataFrame()
    for name, group in grouped_data:
        # group.to_json(f"request_data/{name}.jsonl", orient="records", lines=True)
        if group["evt.type"].iloc[0] != "recvfrom":
            print(f"recvfrom not found: {name}")
            continue
        request_info = group["evt.arg.data"].iloc[0]
        curr_fd = group["fd.name"].iloc[0]
        for i in range(1, len(group)):
            if group["evt.type"].iloc[i] == "recvfrom" and group["fd.name"].iloc[i] == curr_fd:
                request_info += group["evt.arg.data"].iloc[i]
        group = extract_tuple_for_single_request(group)
        group["request_info"] = request_info
        group["request_id"] = name

        aggregated_data = pd.concat([aggregated_data, group])
    
    aggregated_data = pd.concat([aggregated_data, postgres_data])
    # aggregated_data.to_json("all_data.jsonl", orient="records", lines=True)
    def match_id_database(group: pd.DataFrame) -> pd.DataFrame:
        group = group.sort_values("evt.datetime")
        group["request_id"] = group["request_id"].ffill()
        group["request_id"] = group["request_id"].bfill()
        group["request_info"] = group["request_info"].ffill()
        group["request_info"] = group["request_info"].bfill()
        return group
    matched_data = aggregated_data.groupby("client_tuple", sort=False).apply(
        match_id_database, include_groups=True
    ).groupby("proc.name", sort=False).get_group("postgres").sort_values("evt.datetime")
    matched_data = matched_data[matched_data["request_id"].notna()]
    matched_data.to_json("matched_data.jsonl", orient="records", lines=True)
    # 将matched_data中的request_id和statement用字典形式输出, result[request_id] = [statement1, statement2]
    result = {}
    for _, row in matched_data.iterrows():
        if str(row["request_id"]) == "NaN":
            continue
        if row["request_id"] not in result:
            result[row["request_id"]] = {
                "request_info": row["request_info"],
                "statement": []
            }
        if row["statement"]:
            result[row["request_id"]]["statement"].append(row["statement"])

    # 将结果输出为json格式
    with open("db_results.json", "w") as f:
        json.dump(result, f, indent=2)
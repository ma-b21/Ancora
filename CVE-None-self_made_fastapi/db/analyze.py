import re
import pandas as pd
import os
from collections import deque
import json


DEBUG = False

if DEBUG:
    pd.set_option('display.max_columns', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_colwidth', None)


data = pd.read_json("./capture/capture.jsonl", lines=True, dtype={"proc.vpid": str, "proc.cvpid": str, "proc.pvpid": str, "thread.vtid": str, \
                                                                  "fd.cport": str, "fd.sport": str})


def analyze_postgres_data(data: pd.DataFrame) -> pd.DataFrame:
    log_re_detail = re.compile(
        r".*?\[([0-9\.]+\(\d+\))\].*?LOG:\s+.*?:\s*(.*)\.\d+\..*\[[0-9\.]+\(\d+\)\].*?DETAIL:\s+(.*)", re.DOTALL
    )
    log_re = re.compile(
        r".*?\[([0-9\.]+\(\d+\))\].*?LOG:\s+.*?:\s*(.*)\.", re.DOTALL
    )

    data = data.copy()
    # 删除evt.arg.data中不包含LOG:的行和evt.arg.data为nan
    # print(data["evt.arg.data"])
    # data.to_json("test.jsonl", orient="records", lines=True)
    data = data[~data["evt.arg.data"].isna() & data["evt.arg.data"].str.contains("LOG:")]

    def extract_fields(row):
        if "DETAIL:" in row["evt.arg.data"]:
            match = log_re_detail.match(row["evt.arg.data"])
        else:
            match = log_re.match(row["evt.arg.data"])
        if match:
            if len(match.groups()) == 3:
                # LOG: [client_tuple] statement.
                # DETAIL: details
                return pd.Series({
                    "client_tuple": match.group(1),
                    "statement": match.group(2),
                    "details": match.group(3)
                })
            elif len(match.groups()) == 2:
                # LOG: [client_tuple] statement.
                # 没有DETAIL
                return pd.Series({
                    "client_tuple": match.group(1),
                    "statement": match.group(2),
                    "details": None
                })
        return pd.Series({
            "client_tuple": None,
            "statement": None,
            "details": None
        })

    data[["client_tuple", "statement", "details"]] = data.apply(extract_fields, axis=1)
    # 非数据库日志丢弃
    data = data[data["evt.arg.data"].apply(
        lambda x: "LOG:" in x and "execute" in x
    )]
    for index, row in data.iterrows():
        if row["details"] and row["details"].startswith("Parameters: "):
            details_str = row["details"][len("Parameters: "):]
            # print(f"details_str: {details_str}")
            params = [p.split("=", 1) for p in details_str.strip(" .").split(", $")]
            # print(f"params: {params}")
            parameters = {'$' + k.strip('$ '): v.strip() for k, v in params}
            statement = row["statement"]
            for key, value in reversed(parameters.items()):
                statement = statement.replace(key, value)
            data.loc[index, "statement"] = statement
        
        data.loc[index, "statement"] = data.loc[index, "statement"].replace("..", " ")  # 转义单引号

    return data.drop(columns=["details"], errors='ignore').reset_index(drop=True)


def extract_tuple_for_single_request(data: pd.DataFrame) -> pd.DataFrame:
    # 提取数据库连接
    connection_re = re.compile(
        r"(\d+.\d+.\d+.\d+):(\d+)->(\d+.\d+.\d+.\d+):5432"
    )
    # fd.name为None的行补为空字符串
    try:
        data["fd.name"] = data["fd.name"].fillna("NULL")
    except:
        # print(data)
        raise

    data["client_tuple"] = data["fd.name"].apply(
        lambda x: f"{connection_re.match(x).group(1)}({connection_re.match(x).group(2)})"
        if connection_re.match(x) else None
    )
    # data["client_tuple"] = data["client_tuple"].ffill()
    # data["client_tuple"] = data["client_tuple"].bfill()
    data = data[data["client_tuple"].notna()]
    return data


HTTP_METHODS = ["GET", "POST", "HEAD", "PUT", "DELETE", "OPTIONS", "PATCH", "CONNECT", "TRACE"]
def anylyze_python_single_process_data(data: pd.DataFrame) -> dict:
    req_infos = deque()
    request_data_dict: dict[str, list] = {}
    req_info_dict: dict[str, str] = {}
    delimiter_re = re.compile(
        r".*python (request_start|request_init) (\d+-\d+)(.*)", re.DOTALL
    )

    data["evt.arg.data"] = data["evt.arg.data"].fillna("NULL")
    data = data.reset_index(drop=True)
    current_id = None
    for index, row in data.iterrows():
        if row["evt.type"] == "recvfrom" and row["evt.category"] == "net" and row["evt.arg.data"].split(" ")[0] in HTTP_METHODS:
            req_info = row["evt.arg.data"]
            if index < len(data) - 1:
                next_ = data.iloc[index + 1]
                if next_["evt.type"] == "recvfrom" and next_["evt.category"] == "net" and next_["fd.name"] == row["fd.name"]:
                    req_info += next_["evt.arg.data"]
            req_infos.append(req_info)
            continue
        if "request_init" in row["evt.arg.data"]:
            req_id = delimiter_re.match(row["evt.arg.data"]).group(2)
            current_id = req_id
            request_data_dict[req_id] = []
            req_info_dict[req_id] = req_infos.popleft()
            continue
        if "request_start" in row["evt.arg.data"]:
            req_id = delimiter_re.match(row["evt.arg.data"]).group(2)
            # if req_id not in request_data_dict:
            #     print(req_id)
            current_id = req_id
            continue
        
        if current_id is not None and current_id.split("-")[1] != "0":
            try:
                request_data_dict[current_id].append(row)
            except KeyError:
                pass
    
    for key, value in request_data_dict.items():
        request_data_dict[key] = pd.DataFrame(value)
        request_data_dict[key].loc[:,"request_info"] = req_info_dict[key]
        request_data_dict[key].loc[:,"request_id"] = key
    # with open(f"request_data/req_info_{name}.json", "w") as f:
    #     json.dump(req_info_dict, f, indent=2)
    return request_data_dict


def extract_request_syscalls(grouped_data) -> dict:
    request_data_dict: dict[str, pd.DataFrame] = {}

    for _, data in grouped_data:
        # data.to_json(f"request_data/{name}.jsonl", orient="records", lines=True)
        request_data_dict |= anylyze_python_single_process_data(data)
    
    print("requests: ", len(request_data_dict))
    return request_data_dict



if __name__ == "__main__":
    os.system("rm -f ./request_data/*")
    
    proc_group = data.groupby("proc.name", sort=False)
    postgres_data = proc_group.get_group("postgres")
    postgres_data = analyze_postgres_data(postgres_data)
    # postgres_data.to_json("request_data/postgres_data.jsonl", orient="records", lines=True)
    print("postgres_data:", len(postgres_data))

    server_data =  data[~data["proc.name"].isin(["postgres", "mysqld"])]
    grouped_data = server_data.groupby("proc.vpid", sort=False)
    request_data_dict = extract_request_syscalls(grouped_data)
    aggregated_data = pd.DataFrame()

    all_data = []
    for key, value in request_data_dict.items():
        request_data_dict[key] = extract_tuple_for_single_request(value).copy()
        all_data.append(request_data_dict[key])
        # request_data_dict[key].to_json(f"request_data/{key}.jsonl", orient="records", lines=True)
        # aggregated_data = pd.concat([aggregated_data, request_data_dict[key]])
    all_data.append(postgres_data)
    aggregated_data = pd.concat(all_data)
    print("data aggregated!")
    # aggregated_data.to_json("request_data/aggregated_data.jsonl", orient="records", lines=True)
    def match_id_database(group: pd.DataFrame) -> pd.DataFrame:
        group = group.sort_values("evt.datetime")
        group["request_id"] = group["request_id"].ffill()
        group["request_id"] = group["request_id"].bfill()
        group["request_info"] = group["request_info"].ffill()
        group["request_info"] = group["request_info"].bfill()
        # group.to_json(f"request_data/{group.iloc[0]['client_tuple']}.jsonl", orient="records", lines=True)
        return group
    matched_data = aggregated_data.groupby("client_tuple", sort=False).apply(
        match_id_database, include_groups=True
    ).groupby("proc.name", sort=False).get_group("postgres").sort_values("evt.datetime")
    matched_data.to_json("matched_data.jsonl", orient="records", lines=True)
    print("matched_data:", len(matched_data))
    # 将matched_data中的request_id和statement用字典形式输出, result[request_id] = [statement1, statement2]
    result = {}
    for _, row in matched_data.iterrows():
        if str(row["request_id"]) == "NaN":
            continue
        if row["request_id"] not in result:
            result[row["request_id"]] = {
                "request_info": row["request_info"],
                "statement": []
            }
        if row["statement"] and "SERVER_HIT_BIN" not in row["statement"] and "SEQUENCE_VALUE_ITEM" not in row["statement"]:
            result[row["request_id"]]["statement"].append(row["statement"])
    
    # 如果"statement"为空，则删除该条记录
    for key in list(result.keys()):
        if not result[key]["statement"]:
            del result[key]

    print("result:", len(result))
    # 将结果输出为json格式
    with open("db_results.json", "w") as f:
        json.dump(result, f, indent=2)
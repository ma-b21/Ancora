import pandas as pd
from ProvGraph import ProvGraph, ProcNode, FileNode, RequestNode, EdgeType
import os
import json
import re


DEBUG = False
DRAW_GRAPH = False

if DEBUG:
    pd.set_option('display.max_columns', None)
    pd.set_option('display.max_rows', None)
    pd.set_option('display.max_colwidth', None)

total = pd.read_json("./capture/capture.jsonl", lines=True, dtype={"proc.vpid": str, "proc.cvpid": str, "proc.pvpid": str, "thread.vtid": str,
                                                                   "thread.pvtid": str, "thread.cvtid": str})

# 去掉mysql的行
total = total[total["proc.name"] != "mysqld"]

def get_unique_ordered_names(group):  
    # 保持顺序的唯一值提取  
    names = []  
    for name in group['proc.name']:  
        if name not in names:  
            names.append(name)  
    return names 
    

unique_names = total.groupby("thread.vtid", sort=False).apply(get_unique_ordered_names)

for vpid, names in unique_names.items():
    if len(names) == 1:
        continue
    # 把多个名字的进程用->连接起来
    new_name = "->".join(names)
    total.loc[total["thread.vtid"] == vpid, "proc.name"] = new_name

proc_groups = total.groupby("thread.vtid", sort=False)

def get_grouped_data(data: pd.DataFrame) -> dict:
    data = data[data["proc.name"] == "apache2"].copy()
    # python_data["thread.vtid"] = python_data["thread.vtid"].astype(int)
    return data.groupby("thread.vtid", sort=False)


def analyze_request_files(request_id: str, data: pd.DataFrame) -> ProvGraph:
    # 迭代每一行，寻找type=file的行
    graph = ProvGraph()
    graph.add_node(RequestNode(request_id))
    file_data = data[(data["evt.category"] == "file") & (data["fd.name"] != "/dev/null")]
    # print(file_data)
    unmatched_copy = None
    for _, row in file_data.iterrows():
        if row["evt.type"] == "read" or row["evt.type"] == "pread":
            pass
            # graph.add_event_edge(FileNode(row["fd.name"], row["fd.type"] == "directory"), RequestNode(request_id), EdgeType.READ, row["evt.datetime"])
        elif row["evt.type"] == "write" or row["evt.type"] == "pwrite" \
             or ((row["evt.type"] == "open" or row["evt.type"] == "openat") and ("O_F_CREATED" not in row["evt.info"]) and ("O_TRUNC" in row["evt.info"])):
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.WRITE, row["evt.datetime"], row["evt.arg.data"])
        elif row["evt.type"] == "copy_file_range":
            if row["evt.dir"] == ">":
                unmatched_copy = (FileNode(row["fd.name"], row["fd.type"] == "directory"), RequestNode(request_id), EdgeType.COPY_FROM, row["evt.datetime"])
            elif row["evt.dir"] == "<":
                if unmatched_copy is not None:
                    # graph.add_event_edge(*unmatched_copy)
                    graph.add_event_edge(RequestNode(request_id), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.COPY_TO, row["evt.datetime"])
                    graph.add_event_edge(FileNode(unmatched_copy[0].name), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.COPY, row["evt.datetime"])
                    unmatched_copy = None
        elif (row["evt.type"] == "open" or row["evt.type"] == "openat") and "O_F_CREATED" in row["evt.info"]:
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.CREATE, row["evt.datetime"])
        elif (row["evt.type"] == "unlink" or row["evt.type"] == "unlinkat") and row["evt.info"].startswith("res=0"):
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fs.path.name"]), EdgeType.DELETE, row["evt.datetime"])
        elif (row["evt.type"] == "chmod") and row["evt.info"].startswith("res=0"):
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fs.path.name"]), EdgeType.CHMOD, row["evt.datetime"])
        elif (row["evt.type"] == "chown") and row["evt.info"].startswith("res=0"):
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fs.path.name"]), EdgeType.CHOWN, row["evt.datetime"])
        elif row["evt.type"] == "mkdir" and row["evt.info"].startswith("res=0"):
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fs.path.name"]), EdgeType.MKDIR, row["evt.datetime"])
        elif row["evt.type"] == "rmdir" and row["evt.info"].startswith("res=0"):
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fs.path.name"]), EdgeType.RMDIR, row["evt.datetime"])
        elif (row["evt.type"] == "renameat2" or row["evt.type"] == "renameat" or row["evt.type"] == "rename") and row["evt.info"].startswith("res=0"):
            # graph.add_event_edge(FileNode(row["fs.path.source"]), RequestNode(request_id), "rename_source", row["evt.datetime"])
            graph.add_event_edge(RequestNode(request_id), FileNode(row["fs.path.target"]), EdgeType.RENAME_TARGET, row["evt.datetime"])
            graph.add_event_edge(FileNode(row["fs.path.source"]), FileNode(row["fs.path.target"]), EdgeType.RENAME, row["evt.datetime"])
    
    return graph


def analyze_request_subprocesses(data: pd.DataFrame) -> list:
    request_subprocesses = []
    subprocess_data = data[(data["evt.type"] == "clone") | (data["evt.type"] == "vfork")]
    # print(subprocess_data)
    for _, row in subprocess_data.iterrows():
        request_subprocesses.append(row["thread.cvtid"])
    
    return request_subprocesses


def analyze_single_process_data(name: str, group: pd.DataFrame) -> dict:
    # group.to_json(f"request_data/{name}.jsonl", orient="records", lines=True)
    request_data_dict: dict[str, list] = {}
    in_request = False
    request_id = 0
    current_fd = None
    request_info = None
    # group.to_json(f"request_data/{name}.jsonl", orient="records", lines=True)
    for _, row in group.iterrows():
        # 根据accept4和close事件来判断请求的开始和结束
        if row["evt.type"] == "read" and row["evt.category"] == "net" and not row["fd.name"].startswith("127.0.0.1") and row["evt.arg.data"][0].isupper():
            res = re.search(r"res=([1-9]\d*)", row["evt.info"])
            if not res:
                continue
            if in_request:
                request_data_dict[f"{name}-{request_id}"] = pd.DataFrame(request_data_dict[f"{name}-{request_id}"])
                request_data_dict[f"{name}-{request_id}"]["request_info"] = request_info
                request_info = None
            in_request = True
            current_fd = row["fd.name"]
            request_id += 1
            request_data_dict[f"{name}-{request_id}"] = []
        if in_request:
            request_data_dict[f"{name}-{request_id}"].append(row)
            if row["evt.type"] == "read" and row["evt.category"] == "net" and row["fd.name"] == current_fd:
                if not request_info:
                    request_info = ""
                request_info += row["evt.arg.data"]

        if row["evt.type"] == "writev" and row["fd.name"] == current_fd:
            in_request = False
            current_fd = None
            # 把数据转换为DataFrame
            request_data_dict[f"{name}-{request_id}"] = pd.DataFrame(request_data_dict[f"{name}-{request_id}"])
            request_data_dict[f"{name}-{request_id}"]["request_info"] = request_info
            request_info = None
    if len(request_data_dict) == 0:
        print(f"Process {name} has no requests.")
        return None, None
    
    print(f"Process {name} has {len(request_data_dict)} requests.")
    # 按照request_id分组
    result = ({}, {})
    result_info_map = {}
    for request_id, group in request_data_dict.items():
        print(f"Analyzing request {request_id}")
        request_graph = analyze_request_files(request_id, group)
        result[0][request_id] = request_graph
        request_subprocesses = analyze_request_subprocesses(group)
        result[1][request_id] = request_subprocesses
        result_info_map[request_id] = group.iloc[0]["request_info"]

    return result, result_info_map


def build_graph(request_id: str, vpids: list, graph: ProvGraph | None = None) -> ProvGraph:
    graph = ProvGraph() if graph is None else graph
    graph.add_node(RequestNode(request_id))
    unmatched_copy = None
    for vpid in vpids:
        try:
            proc_data = proc_groups.get_group(vpid)
            graph.add_event_edge(RequestNode(request_id), ProcNode(vpid, proc_data.iloc[0]["proc.name"]), EdgeType.CREATE_SUBPROCESS, proc_data.iloc[0]["evt.datetime"])
        except KeyError:
            # graph.add_event_edge(RequestNode(request_id), ProcNode(vpid, "unknown"), EdgeType.CREATE_SUBPROCESS, proc_data.iloc[0]["evt.datetime"])
            continue
    while len(vpids) > 0:
        vpid = vpids.pop()
        try:
            proc_data = proc_groups.get_group(vpid)
        except KeyError:
            continue
        for _, row in proc_data.iterrows():
            if row["evt.category"] == "file":
                if row["evt.type"] == "read" or row["evt.type"] == "pread":
                    pass
                    # graph.add_event_edge(FileNode(row["fd.name"], row["fd.type"] == "directory"), ProcNode(vpid, proc_data.iloc[0]["proc.name"]), EdgeType.READ, row["evt.datetime"])
                elif row["evt.type"] == "write" or row["evt.type"] == "pwrite" \
                    or ((row["evt.type"] == "open" or row["evt.type"] == "openat") and ("O_F_CREATED" not in row["evt.info"]) and ("O_TRUNC" in row["evt.info"])):
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.WRITE, row["evt.datetime"], row["evt.arg.data"])
                elif row["evt.type"] == "copy_file_range":
                    if row["evt.dir"] == ">":
                        unmatched_copy = (FileNode(row["fd.name"], row["fd.type"] == "directory"), ProcNode(vpid, proc_data.iloc[0]["proc.name"]), EdgeType.COPY_FROM, row["evt.datetime"])
                    elif row["evt.dir"] == "<":
                        if unmatched_copy is not None:
                            # graph.add_event_edge(*unmatched_copy)
                            graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.COPY_TO, row["evt.datetime"])
                            graph.add_event_edge(FileNode(unmatched_copy[0].name), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.COPY, row["evt.datetime"])
                            unmatched_copy = None
                elif (row["evt.type"] == "open" or row["evt.type"] == "openat") and "O_F_CREATED" in row["evt.info"]:
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fd.name"], row["fd.type"] == "directory"), EdgeType.CREATE, row["evt.datetime"])
                elif (row["evt.type"] == "unlink" or row["evt.type"] == "unlinkat") and row["evt.info"].startswith("res=0"):
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fs.path.name"]), EdgeType.DELETE, row["evt.datetime"])
                elif (row["evt.type"] == "chmod") and row["evt.info"].startswith("res=0"):
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fs.path.name"]), EdgeType.CHMOD, row["evt.datetime"])
                elif (row["evt.type"] == "chown") and row["evt.info"].startswith("res=0"):
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fs.path.name"]), EdgeType.CHOWN, row["evt.datetime"])
                elif row["evt.type"] == "mkdir" and row["evt.info"].startswith("res=0"):
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fs.path.name"]), EdgeType.MKDIR, row["evt.datetime"])
                elif row["evt.type"] == "rmdir" and row["evt.info"].startswith("res=0"):
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fs.path.name"]), EdgeType.RMDIR, row["evt.datetime"])
                elif (row["evt.type"] == "renameat2" or row["evt.type"] == "renameat" or row["evt.type"] == "rename") and row["evt.info"].startswith("res=0"):
                    # graph.add_event_edge(FileNode(row["fs.path.source"]), ProcNode(vpid, proc_data.iloc[0]["proc.name"]), "rename_source", row["evt.datetime"])
                    graph.add_event_edge(ProcNode(vpid, proc_data.iloc[0]["proc.name"]), FileNode(row["fs.path.target"]), EdgeType.RENAME_TARGET, row["evt.datetime"])
                    graph.add_event_edge(FileNode(row["fs.path.source"]), FileNode(row["fs.path.target"]), EdgeType.RENAME, row["evt.datetime"])
            if row["evt.type"] == "clone" or row["evt.type"] == "vfork":
                try:
                    cproc_data = proc_groups.get_group(row["thread.cvtid"])
                    vpids.append(row["thread.cvtid"])
                    graph.add_event_edge(ProcNode(vpid, row["proc.name"]), ProcNode(row["thread.cvtid"], cproc_data.iloc[0]["proc.name"]), EdgeType.CREATE_SUBPROCESS, row["evt.datetime"])
                except KeyError:
                    graph.add_event_edge(ProcNode(vpid, row["proc.name"]), ProcNode(row["thread.cvtid"], "unknown"), EdgeType.CREATE_SUBPROCESS, row["evt.datetime"])
    return graph
            

def analyze_results():
    results = {}
    req_info = {}
    matchare_res = {}
    total_requests = 0


    grouped_data = get_grouped_data(total)
    os.system("rm -rf ./prov_graphs/*")
    os.system("rm -rf ./request_data/*")
    # file_data = total[(total["evt.category"] == "file")]
    # # 去掉fd.name为NaN的行
    # file_data = file_data[file_data["fd.name"].notna()]
    # # 只保留fd.name 以/var/www/html/images/data/开头的行
    # file_data = file_data[file_data["fd.name"].str.startswith("/var/www/html/images/data/")]
    # file_data.to_json("request_data/file_data.jsonl", orient="records", lines=True)

    op_len = 0
    for name, group in grouped_data:
        print(f"Analyzing process {name}")
        analyzed_data, req_info_map = analyze_single_process_data(name, group)
        if analyzed_data is not None:
            req_info |= req_info_map
            total_requests += len(analyzed_data[0])
            for request_id, vpids in analyzed_data[1].items():
                graph = analyzed_data[0][request_id]
                graph = build_graph(request_id, vpids, graph)
                if DRAW_GRAPH:
                    graph.draw_graph(file_name=f"prov_graph_{request_id}", show=False, save_path="./prov_graphs/")
                affected_files = graph.find_affected_files(RequestNode(request_id))
                results[request_id] = affected_files
                for _, value in affected_files.items():
                    op_len += len(value)
                matchare_res[request_id] = graph.find_related_files(RequestNode(request_id))

                
                # 排除临时文件
                for file in matchare_res[request_id]:
                    if file["operation"] == "move":
                        file["operation"] = "create"
                        file["source_path"] = file["destination_path"]
                        file["destination_path"] = None
                matchare_res[request_id] = [file for file in matchare_res[request_id] if not file["source_path"].startswith("/tmp/") and not file["source_path"].startswith("/var/www/html/tmp/")]


    print(f"Total requests: {total_requests}")
    print("Total File Operations: ", op_len)
    print("Done!")
    with open("fs_results.json", "w") as out:
        out.write(json.dumps(results, indent=4))
    with open("req_info.json", "w") as out:
        out.write(json.dumps(req_info, indent=4))
    with open("matchare_res.json", "w") as out:
        out.write(json.dumps(matchare_res, indent=4))

    return results

if __name__ == "__main__":
    analyze_results()
    pass
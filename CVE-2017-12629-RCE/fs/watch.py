from watchdog import observers  
from watchdog.events import FileSystemEventHandler
from loguru import logger  
from datetime import datetime, timedelta  
import subprocess  
import time  
import docker  
import threading  
import queue

import os

from config import hook_containers, src_dir, backup_dir, env


client = docker.from_env()
fs_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
hook_dir = os.path.join(fs_dir, "hook")  
hook_log_dir = os.path.join(hook_dir, "log")  

def hook_kernel():  
    pids = [str(client.containers.get(container).attrs["State"]["Pid"]) for container in hook_containers]  
    hook_h = f'''  
#ifndef LOG_HOOK_H  
#define LOG_HOOK_H  
#define CONTAINER_NUMS 1  
#define LOG_DIR "{hook_log_dir}"  
const int container_pids[] = {{{",".join(pids)}}};  
const char container_ids[][64] = {{{",".join([f'"{container}"' for container in hook_containers])}}};  
#endif  
'''  
    with open(f"{hook_dir}/log_hook.h", "w") as f:  
        f.write(hook_h)  

    subprocess.run(["make"], shell=True, cwd=hook_dir, check=True)  
    subprocess.run(["sudo", "insmod", "rootkit.ko"], cwd=hook_dir, check=True)  

def unhook_kernel():  
    subprocess.run(["sudo", "rmmod", "rootkit"], cwd=hook_dir, check=True)  

class SyncHandler(FileSystemEventHandler):  
    def __init__(self, src_path, dest_path):  
        # 确保源路径存在且为目录  
        if not os.path.exists(src_path):  
            raise FileNotFoundError(f"Path {src_path} not found")  
        if not os.path.isdir(src_path):  
            raise NotADirectoryError(f"Path {src_path} is not a directory")  
        
        # 建立备份路径  
        os.makedirs(dest_path, exist_ok=True)  
        self.data_path = src_path  
        self.backup_path = dest_path  
        
        # 备份队列和线程  
        self.backup_queue = queue.Queue()  
        self.backup_thread = threading.Thread(target=self._backup_worker, daemon=True)  
        self.backup_thread.start()  
        
        # 最后一次备份的时间戳  
        self.last_backup_time = 0  
        
        # 备份锁，防止并发备份  
        self.backup_lock = threading.Lock()
        # 增加一个延迟备份的定时器  
        self.delayed_backup_timer = None 
        
        # 初始备份  
        self.trigger_backup()  
        
        super().__init__()  
    
    def trigger_backup(self):  
        """  
        触发备份的方法，通过队列进行去重和限流  
        """  
        current_time = time.time()

        # 如果已经有延迟定时器，取消它  
        if self.delayed_backup_timer:  
            self.delayed_backup_timer.cancel() 
        
        # 如果距离上次备份不到1秒，则不执行  
        if current_time - self.last_backup_time < 1:
            # 创建一个定时器，1秒后执行备份  
            self.delayed_backup_timer = threading.Timer(1, self._delayed_backup)  
            self.delayed_backup_timer.start()  
            return  
        
        # 将备份任务放入队列  
        self.backup_queue.put(True)  

    def _delayed_backup(self):  
        """  
        延迟备份的方法  
        """  
        self.backup_queue.put(True)  
        self.delayed_backup_timer = None  

    def _backup_worker(self):  
        """  
        后台备份工作线程  
        """  
        while True:  
            # 等待备份信号  
            self.backup_queue.get()  
            
            try:  
                with self.backup_lock:  
                    current_time = time.time()  
                    
                    # 再次检查时间间隔  
                    if current_time - self.last_backup_time < 1:  
                        continue
                    
                    # 执行备份脚本  
                    subprocess.run(  
                        f"sudo ./{env}/fs/backup.sh -s {self.data_path} -d {self.backup_path}", # -t '{timestamp}'",  
                        shell=True,   
                        check=True  
                    )  
                    
                    # 更新最后备份时间  
                    self.last_backup_time = current_time  
                    
                    # 清理过期备份  
                    self._clean_old_backups()  
                    
            except Exception as e:  
                logger.error(f"Backup failed: {e}")  
            finally:  
                # 标记任务完成  
                self.backup_queue.task_done()  
    
    def _clean_old_backups(self):  
        """  
        清理30天前的备份  
        """  
        outdate_time = datetime.now() - timedelta(days=30)  
        outdate_time_str = outdate_time.strftime("%Y-%m-%d_%H-%M-%S.%f")  
        
        backups = os.listdir(self.backup_path)  
        for backup in backups:  
            if backup < outdate_time_str:  
                subprocess.run(  
                    f"rm -rf {os.path.join(self.backup_path, backup)}",  
                    shell=True  
                )  
    
    def on_created(self, event):  
        self.trigger_backup()  
        logger.debug(f"File {event.src_path} created")  
        return super().on_created(event)  
    
    def on_deleted(self, event):  
        self.trigger_backup()  
        logger.debug(f"File {event.src_path} deleted")  
        return super().on_deleted(event)  

    def on_modified(self, event):  
        if event.is_directory:  
            return  
        self.trigger_backup()  
        logger.debug(f"File {event.src_path} modified")  
        return super().on_modified(event)  
    
    def on_moved(self, event):  
        self.trigger_backup()  
        logger.debug(f"File {event.src_path} moved to {event.dest_path}")  
        return super().on_moved(event)  
    
    def on_any_event(self, event):  
        # 忽略常见的临时文件  
        if event.src_path.endswith('.swp') or event.src_path.endswith('.swx'):  
            return  
        return super().on_any_event(event)  

if __name__ == '__main__':  
    hook_kernel()  
    event_handler = SyncHandler(src_dir, backup_dir)
    observer = observers.Observer()  
    observer.schedule(event_handler, path=src_dir, recursive=True)
    observer.start()  

    try:  
        while True:  
            time.sleep(5)  
    except KeyboardInterrupt:  
        observer.stop()  
    observer.join()  
    print("Watchdog stopped")  
    unhook_kernel()  
    print("Unhook kernel module")